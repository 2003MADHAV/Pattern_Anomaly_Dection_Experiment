{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQqi+QtVOrcEe3KV4jgiTz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":931,"output_embedded_package_id":"1HTCHIck1rrOsGPEESb8H4FZYpOlq_OqY"},"id":"wEcRgouIulEM","executionInfo":{"status":"ok","timestamp":1707734322531,"user_tz":480,"elapsed":3664,"user":{"displayName":"MADHAV KUMAR 2021-CSE BATCH","userId":"09910938514947058271"}},"outputId":"eb524a17-16c5-4df1-beda-f8c022f9c1e9"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import cv2\n","import numpy as np\n","from keras.models import model_from_json\n","from google.colab.patches import cv2_imshow\n","\n","emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n","\n","json_file = open('/content/emotion_model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","emotion_model = model_from_json(loaded_model_json)\n","emotion_model.load_weights(\"/content/emotion_model.h5\")\n","print(\"Loaded model from disk\")\n","\n","\n","input_image_path=\"/content/girl.jpg\"\n","\n","frame = cv2.imread(input_image_path)\n","\n","if frame is not None:\n","    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_default.xml')\n","    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n","\n","    for (x, y, w, h) in num_faces:\n","        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n","        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n","        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n","\n","        emotion_prediction = emotion_model.predict(cropped_img)\n","        maxindex = int(np.argmax(emotion_prediction))\n","\n","        confidence_threshold = 0.5\n","\n","        if emotion_prediction[0][maxindex] > confidence_threshold:\n","            cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n","\n","\n","    cv2_imshow(frame)\n","    print(frame.shape)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","else:\n","    print(f\"Error: Unable to load the image at '{input_image_path}'\")"]}]}